{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPOteEo9inKnSMTfiNXcAxI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"l6_Van0vjpw_","executionInfo":{"status":"ok","timestamp":1664460272678,"user_tz":-330,"elapsed":1114,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}}},"outputs":[],"source":["#Importing Pandas Library\n","import pandas as pd"]},{"cell_type":"code","source":["#Reading the dataset\n","df = pd.read_csv(\"https://raw.githubusercontent.com/analyticsindiamagazine/MocksDatasets/main/food_review.csv\")"],"metadata":{"id":"ga0Lbd9Mj_4v","executionInfo":{"status":"ok","timestamp":1664460272678,"user_tz":-330,"elapsed":7,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["#Visualizing Data\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"kimAzhjokBbT","executionInfo":{"status":"ok","timestamp":1664460272679,"user_tz":-330,"elapsed":7,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"aa89853b-e23c-45fe-c811-954c6d9e8b33"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                        review  reaction\n","0            Service is friendly and inviting.         1\n","1                    Awesome service and food.         1\n","2       Waitress was a little slow in service.         0\n","3        Come hungry, leave happy and stuffed!         1\n","4  Horrible - don't waste your time and money.         0"],"text/html":["\n","  <div id=\"df-835b1500-cc68-43cf-a401-ce349d93e8f6\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>reaction</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Service is friendly and inviting.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Awesome service and food.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Waitress was a little slow in service.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Come hungry, leave happy and stuffed!</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Horrible - don't waste your time and money.</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-835b1500-cc68-43cf-a401-ce349d93e8f6')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-835b1500-cc68-43cf-a401-ce349d93e8f6 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-835b1500-cc68-43cf-a401-ce349d93e8f6');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["# **Data Preprocessing**"],"metadata":{"id":"PYl3zAlBkDK8"}},{"cell_type":"code","source":["#Visualizing the Shape of the data \n","df.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yE65NpmkkFwL","executionInfo":{"status":"ok","timestamp":1664460290816,"user_tz":-330,"elapsed":3,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"9374b71c-cd01-4de5-fbf9-18f358a1da31"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1000, 2)"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["df[\"reaction\"].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4bRsgSPdkHVk","executionInfo":{"status":"ok","timestamp":1664460296521,"user_tz":-330,"elapsed":3,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"7b289085-f96c-49dd-dfcc-e7b82964c58e"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1    500\n","0    500\n","Name: reaction, dtype: int64"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["df[\"reaction\"].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iBdlsu2RkIrb","executionInfo":{"status":"ok","timestamp":1664460301789,"user_tz":-330,"elapsed":2,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"fa71ec01-ea20-4968-c5d6-3cba9c7e083e"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1    500\n","0    500\n","Name: reaction, dtype: int64"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["#Checking for NULL values\n","df.isnull().sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ClFBgdgEkKAC","executionInfo":{"status":"ok","timestamp":1664460307963,"user_tz":-330,"elapsed":4,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"238fafbc-d644-4365-8088-b7ab340aa39f"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["review      0\n","reaction    0\n","dtype: int64"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["#Checking for NA values\n","df.isna().sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xYW4R1vVkLa1","executionInfo":{"status":"ok","timestamp":1664460313661,"user_tz":-330,"elapsed":6,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"0da987af-c1f1-48b8-b955-74b2155e0803"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["review      0\n","reaction    0\n","dtype: int64"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["#Checking for duplicate values\n","print(\"Total Number of duplicated:\",df.duplicated().sum())\n","print(\"Shape of Data:\",df.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H3kUuYVGkM7O","executionInfo":{"status":"ok","timestamp":1664460318900,"user_tz":-330,"elapsed":6,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"b3ea676a-5d80-41a1-a968-3f028743bafa"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Total Number of duplicated: 4\n","Shape of Data: (1000, 2)\n"]}]},{"cell_type":"code","source":["#Removing duplicate values \n","df.drop_duplicates(inplace = True)\n","print(\"Total Number of duplicated:\",df.duplicated().sum())\n","print(\"Shape of Data:\",df.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qEZm-4YOkOIn","executionInfo":{"status":"ok","timestamp":1664460324691,"user_tz":-330,"elapsed":914,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"7558c48c-67ae-4c7c-9232-0aacc27ccf9b"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Total Number of duplicated: 0\n","Shape of Data: (996, 2)\n"]}]},{"cell_type":"markdown","source":["# **BINARY ENCODER**"],"metadata":{"id":"q1wS5KXDkP4b"}},{"cell_type":"markdown","source":["First, import the CountVectorizer from the sklearn library to perform vectorization of the texts. First, convert all characters to lowercase before tokenizing by setting the default parameter lowercase = True, and to provide a binary label to each of the unique words use another default parameter binary = True. By setting ‘binary= True’, the CountVectorizer does not count the frequency of the word but it represents 1 if the unique word is present in the text sample and 0 if the unique word is not present in the text sample.  "],"metadata":{"id":"d8kfIOCYkdVV"}},{"cell_type":"code","source":["#Importing CountVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","#Vectorization of input variables using count vectorizer\n","cv = CountVectorizer(binary = True,lowercase = True)\n","X = cv.fit_transform(df[\"review\"].values)"],"metadata":{"id":"iMGkjVp5kSTS","executionInfo":{"status":"ok","timestamp":1664460393248,"user_tz":-330,"elapsed":995,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["After transforming the textual data into vectors, using the pandas library we are creating a DataFrame that represents all the unique words in the columns and all the reviews in the rows. The parameter todense()  returns a matrix of the given series vectors.  "],"metadata":{"id":"TWQeQy3_ki7l"}},{"cell_type":"code","source":["X = pd.DataFrame(X.todense(),columns= cv.get_feature_names())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TMNQ6xpzkf5z","executionInfo":{"status":"ok","timestamp":1664460413613,"user_tz":-330,"elapsed":698,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"1ffd83b5-c695-4aa5-a3b7-576839c760e5"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]}]},{"cell_type":"code","source":["#Input Variable\n","X"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"l7VsSdFMklQQ","executionInfo":{"status":"ok","timestamp":1664460421573,"user_tz":-330,"elapsed":914,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"abccda17-67ed-4e6a-ddbb-57c0c7a53ed9"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     00  10  100  11  12  15  17  1979  20  2007  ...  yelpers  yet  you  \\\n","0     0   0    0   0   0   0   0     0   0     0  ...        0    0    0   \n","1     0   0    0   0   0   0   0     0   0     0  ...        0    0    0   \n","2     0   0    0   0   0   0   0     0   0     0  ...        0    0    0   \n","3     0   0    0   0   0   0   0     0   0     0  ...        0    0    0   \n","4     0   0    0   0   0   0   0     0   0     0  ...        0    0    0   \n","..   ..  ..  ...  ..  ..  ..  ..   ...  ..   ...  ...      ...  ...  ...   \n","991   0   0    0   0   0   0   0     0   0     0  ...        0    0    0   \n","992   0   0    0   0   0   0   0     0   0     0  ...        0    0    0   \n","993   0   0    0   0   0   0   0     0   0     0  ...        0    0    0   \n","994   0   0    0   0   0   0   0     0   0     0  ...        0    0    0   \n","995   0   0    0   0   0   0   0     0   0     0  ...        0    0    0   \n","\n","     your  yourself  yucky  yukon  yum  yummy  zero  \n","0       0         0      0      0    0      0     0  \n","1       0         0      0      0    0      0     0  \n","2       0         0      0      0    0      0     0  \n","3       0         0      0      0    0      0     0  \n","4       1         0      0      0    0      0     0  \n","..    ...       ...    ...    ...  ...    ...   ...  \n","991     0         0      0      0    0      0     0  \n","992     0         0      0      0    0      0     0  \n","993     0         0      0      0    0      0     0  \n","994     0         0      0      0    0      0     0  \n","995     0         0      0      0    0      0     0  \n","\n","[996 rows x 2035 columns]"],"text/html":["\n","  <div id=\"df-5357defd-c9af-4fed-8c9d-0063b1230a12\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>00</th>\n","      <th>10</th>\n","      <th>100</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>15</th>\n","      <th>17</th>\n","      <th>1979</th>\n","      <th>20</th>\n","      <th>2007</th>\n","      <th>...</th>\n","      <th>yelpers</th>\n","      <th>yet</th>\n","      <th>you</th>\n","      <th>your</th>\n","      <th>yourself</th>\n","      <th>yucky</th>\n","      <th>yukon</th>\n","      <th>yum</th>\n","      <th>yummy</th>\n","      <th>zero</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>991</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>992</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>993</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>994</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>995</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>996 rows × 2035 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5357defd-c9af-4fed-8c9d-0063b1230a12')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-5357defd-c9af-4fed-8c9d-0063b1230a12 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-5357defd-c9af-4fed-8c9d-0063b1230a12');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["# **Bag of Words**"],"metadata":{"id":"6CT1atuAknrH"}},{"cell_type":"markdown","source":["CountVectorizer is a tool provided by the scikit-learn library which is used to transform a given text into a vector on the basis of the frequency or count of each word that occurs in the entire text. In-text analysis, Countvectorizer is helpful to convert each word in each text into vectors. CountVectorizer creates a matrix in which each unique word is represented in a column of the matrix and each of the text from the document is a row in the matrix. The value of each cell is the count of the word in the particular text sample. \n","\n","First, import the CountVectorizer from the sklearn library to perform vectorization of the texts.\n","\n","First, convert all characters to lowercase before tokenizing by setting the default parameter lowercase = True, and to count the frequency of each of the unique words use another default parameter binary = Fasle.\n","\n"],"metadata":{"id":"UL5lmiN9kwGj"}},{"cell_type":"code","source":["#Importing CountVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","#Vectorization of input variables using count vectorizer\n","cv = CountVectorizer(binary =False,lowercase = True)\n","X = cv.fit_transform(df[\"review\"].values)"],"metadata":{"id":"z1F4Y85Nkwqj","executionInfo":{"status":"ok","timestamp":1664460488559,"user_tz":-330,"elapsed":765,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["X = pd.DataFrame(X.todense(),columns= cv.get_feature_names())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iS6NJNbWkyA_","executionInfo":{"status":"ok","timestamp":1664460495663,"user_tz":-330,"elapsed":12,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"d178ab97-7f3a-4a64-a736-1e6bb3e09675"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]}]},{"cell_type":"code","source":["#Input Variable\n","X"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"clEsh3BAk5CQ","executionInfo":{"status":"ok","timestamp":1664460510297,"user_tz":-330,"elapsed":3,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"4e83564f-906d-4902-e88c-7809eefd8d0e"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     00  10  100  11  12  15  17  1979  20  2007  ...  yelpers  yet  you  \\\n","0     0   0    0   0   0   0   0     0   0     0  ...        0    0    0   \n","1     0   0    0   0   0   0   0     0   0     0  ...        0    0    0   \n","2     0   0    0   0   0   0   0     0   0     0  ...        0    0    0   \n","3     0   0    0   0   0   0   0     0   0     0  ...        0    0    0   \n","4     0   0    0   0   0   0   0     0   0     0  ...        0    0    0   \n","..   ..  ..  ...  ..  ..  ..  ..   ...  ..   ...  ...      ...  ...  ...   \n","991   0   0    0   0   0   0   0     0   0     0  ...        0    0    0   \n","992   0   0    0   0   0   0   0     0   0     0  ...        0    0    0   \n","993   0   0    0   0   0   0   0     0   0     0  ...        0    0    0   \n","994   0   0    0   0   0   0   0     0   0     0  ...        0    0    0   \n","995   0   0    0   0   0   0   0     0   0     0  ...        0    0    0   \n","\n","     your  yourself  yucky  yukon  yum  yummy  zero  \n","0       0         0      0      0    0      0     0  \n","1       0         0      0      0    0      0     0  \n","2       0         0      0      0    0      0     0  \n","3       0         0      0      0    0      0     0  \n","4       1         0      0      0    0      0     0  \n","..    ...       ...    ...    ...  ...    ...   ...  \n","991     0         0      0      0    0      0     0  \n","992     0         0      0      0    0      0     0  \n","993     0         0      0      0    0      0     0  \n","994     0         0      0      0    0      0     0  \n","995     0         0      0      0    0      0     0  \n","\n","[996 rows x 2035 columns]"],"text/html":["\n","  <div id=\"df-4267a806-391a-4eeb-8474-8cd187e494d7\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>00</th>\n","      <th>10</th>\n","      <th>100</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>15</th>\n","      <th>17</th>\n","      <th>1979</th>\n","      <th>20</th>\n","      <th>2007</th>\n","      <th>...</th>\n","      <th>yelpers</th>\n","      <th>yet</th>\n","      <th>you</th>\n","      <th>your</th>\n","      <th>yourself</th>\n","      <th>yucky</th>\n","      <th>yukon</th>\n","      <th>yum</th>\n","      <th>yummy</th>\n","      <th>zero</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>991</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>992</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>993</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>994</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>995</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>996 rows × 2035 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4267a806-391a-4eeb-8474-8cd187e494d7')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-4267a806-391a-4eeb-8474-8cd187e494d7 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-4267a806-391a-4eeb-8474-8cd187e494d7');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["# **Word embedding using TF-IDF**"],"metadata":{"id":"-Qo17z3tk-hK"}},{"cell_type":"markdown","source":["The count vectorizer faces two main drawbacks i.e overall document weightage issue and the inability to deal with contextual stopwords. In order to overcome these issues faced by the CountVectorizer, the TF-IDF (Term Frequency Inverse Document Frequency) word embedding technique is adopted which could potentially overcome the problem of the weightage as well as contextual stopwords. The Term-Frequency is used to resolve the weightage issue and respectively Inverse Document Frequency is used to resolve the problem of contextual stopwords. \n","\n","TF-IDF is broken down into two parts TF(Term Frequency) and IDF(Inverse Document Frequency). Term Frequency uses row normalization ( L1 or L2 ) to overcome the weightage program. Inverse Document Frequency tries to come up with a weightage factor for each of the unique words i.e provides a score for each of the unique words. For potential contextual stopwords, the score is high and the score is low for non-stop words i.e we minimize the weightage of the frequent terms.  After computing the TF and IDF we multiply these values together to obtain the TF-IDF value. The important (non-frequent) words have higher TF-IDF scores and corresponding low TF-IDF scores for less important or relevant words.  \n","\n","First, import the TfidfVectorizer from the sklearn library to perform vectorization of the texts.\n","\n","First, convert all characters to lowercase before tokenizing by setting the default parameter lowercase = True, and to count the frequency of each of the unique words use another default parameter binary = Fasle."],"metadata":{"id":"RK2295AllBmn"}},{"cell_type":"code","source":["#Importing Term Frequency-Inverse Document Frequency\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","#Vectorization of input variables using TF-IDF\n","tv = TfidfVectorizer()\n","X = tv.fit_transform(df[\"review\"].values)"],"metadata":{"id":"uEh4iM9Xk8r0","executionInfo":{"status":"ok","timestamp":1664460732010,"user_tz":-330,"elapsed":897,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["X = pd.DataFrame(X.todense(),columns=tv.get_feature_names())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kuZM0as_lyxn","executionInfo":{"status":"ok","timestamp":1664460736432,"user_tz":-330,"elapsed":3,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"6fe67dfc-b87e-43ac-93a9-40317125c490"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]}]},{"cell_type":"code","source":["#Input Variable\n","X"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"kRXOv_tBl0Fs","executionInfo":{"status":"ok","timestamp":1664460742303,"user_tz":-330,"elapsed":843,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"6be8b8df-1196-4811-c860-189810c6cd63"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      00   10  100   11   12   15   17  1979   20  2007  ...  yelpers  yet  \\\n","0    0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0   0.0  ...      0.0  0.0   \n","1    0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0   0.0  ...      0.0  0.0   \n","2    0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0   0.0  ...      0.0  0.0   \n","3    0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0   0.0  ...      0.0  0.0   \n","4    0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0   0.0  ...      0.0  0.0   \n","..   ...  ...  ...  ...  ...  ...  ...   ...  ...   ...  ...      ...  ...   \n","991  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0   0.0  ...      0.0  0.0   \n","992  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0   0.0  ...      0.0  0.0   \n","993  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0   0.0  ...      0.0  0.0   \n","994  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0   0.0  ...      0.0  0.0   \n","995  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0   0.0  ...      0.0  0.0   \n","\n","     you      your  yourself  yucky  yukon  yum  yummy  zero  \n","0    0.0  0.000000       0.0    0.0    0.0  0.0    0.0   0.0  \n","1    0.0  0.000000       0.0    0.0    0.0  0.0    0.0   0.0  \n","2    0.0  0.000000       0.0    0.0    0.0  0.0    0.0   0.0  \n","3    0.0  0.000000       0.0    0.0    0.0  0.0    0.0   0.0  \n","4    0.0  0.353539       0.0    0.0    0.0  0.0    0.0   0.0  \n","..   ...       ...       ...    ...    ...  ...    ...   ...  \n","991  0.0  0.000000       0.0    0.0    0.0  0.0    0.0   0.0  \n","992  0.0  0.000000       0.0    0.0    0.0  0.0    0.0   0.0  \n","993  0.0  0.000000       0.0    0.0    0.0  0.0    0.0   0.0  \n","994  0.0  0.000000       0.0    0.0    0.0  0.0    0.0   0.0  \n","995  0.0  0.000000       0.0    0.0    0.0  0.0    0.0   0.0  \n","\n","[996 rows x 2035 columns]"],"text/html":["\n","  <div id=\"df-da408a94-d583-4e3f-bd2b-53060236ea59\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>00</th>\n","      <th>10</th>\n","      <th>100</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>15</th>\n","      <th>17</th>\n","      <th>1979</th>\n","      <th>20</th>\n","      <th>2007</th>\n","      <th>...</th>\n","      <th>yelpers</th>\n","      <th>yet</th>\n","      <th>you</th>\n","      <th>your</th>\n","      <th>yourself</th>\n","      <th>yucky</th>\n","      <th>yukon</th>\n","      <th>yum</th>\n","      <th>yummy</th>\n","      <th>zero</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.353539</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>991</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>992</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>993</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>994</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>995</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>996 rows × 2035 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-da408a94-d583-4e3f-bd2b-53060236ea59')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-da408a94-d583-4e3f-bd2b-53060236ea59 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-da408a94-d583-4e3f-bd2b-53060236ea59');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["# **BERT**"],"metadata":{"id":"Qbts72pvl11A"}},{"cell_type":"markdown","source":["The BERT model can understand the context of the statement and can generate meaningful vector representations of the given word. BERT can also generate an embedding for the entire sentence. It generates a single vector for the entire sentence. Usually, a BERT model will generate a vector of size 768 dimensions. BERT is based on a transformer architecture that is widely used in the NLP domain. There are two models versions of BERT :\n","\n","BERT Base\n","\n","BERT Large \n","\n","BERT Base - Comparable in size to the OpenAI Transformer in order to compare performance. The base version contains 12 encoding layers, 768 feedforward hidden units, and 12 attention heads. \n","\n","BERT large - A ridiculously huge model which is made up of 24 encoding layers, 1024 feedforward hidden units, and 16 attention heads. "],"metadata":{"id":"DJLLsfd-l7nz"}},{"cell_type":"markdown","source":["Model Inputs: The first input token is supplied with a special [CLS] token for reasons that will become apparent later on. CLS here stands for Classification.BERT takes a sequence of words as input which keep flowing up the stack. Each layer applies self-attention, passes its results through a feed-forward network, and then hands it off to the next encoder.\n","\n","Model Outputs: Each position outputs a vector of size hidden_size (768 in BERT Base).\n","\n"," \n","\n","BERT was trained by Google on 2500 million words in Wikipedia and 800 million words on different books. The Google trained BERT using two approaches:\n","\n","Masked Language model \n","\n","Next sentence prediction  \n","\n"," \n","\n","Now we perform word embedding for the food review dataset using the BERT model. \n","\n","The BERT model has two steps in the process:\n","\n","BERT Preprocessing\n","\n","BERT Embedding \n","\n","Let's try to locate the BERT model on the Tensor flow hub website. The tensor flow hub is a repository of all the trained machine learning models. We are going to use the BERT Base model which has 12 encoders. "],"metadata":{"id":"3-czQcztmETc"}},{"cell_type":"code","source":["#Importing BERT Libraries\n","!pip3 install --quiet tensorflow-text\n","import tensorflow_hub as hub\n","import tensorflow_text as text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PIoqt-Vcl53M","executionInfo":{"status":"ok","timestamp":1664460943579,"user_tz":-330,"elapsed":113015,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"30ae8fec-0235-4494-eb3c-f769fba2fdd3"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 5.9 MB 2.0 MB/s \n","\u001b[K     |████████████████████████████████| 578.0 MB 14 kB/s \n","\u001b[K     |████████████████████████████████| 1.7 MB 45.3 MB/s \n","\u001b[K     |████████████████████████████████| 438 kB 42.3 MB/s \n","\u001b[K     |████████████████████████████████| 5.9 MB 24.5 MB/s \n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["We can directly use the URL from the Tensor flow hub to download the model to the working directory. So download the encoder model using the encoder URL and for each of the encoding models, there is a corresponding preprocessing model which can be downloaded using a preprocessing URL present in the Tensor flow hub. "],"metadata":{"id":"MeAh-28_mMAH"}},{"cell_type":"code","source":["#BERT Preprocessing URL\n","preprocess_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n","#BERT Encoding URL\n","encoder_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\""],"metadata":{"id":"TzzsBUw-mIhM","executionInfo":{"status":"ok","timestamp":1664460943580,"user_tz":-330,"elapsed":10,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["# BERT Preprocessing "],"metadata":{"id":"vgHeuSsLmS8Q"}},{"cell_type":"markdown","source":["So create a preprocessing layer that can certainly perform preprocessing of the textual data. The output of the preprocessing layer is in the form of a dictionary, so visualize the keys of the dictionary to understand the operation of the preprocessing layer. "],"metadata":{"id":"8pl1blTYmWH8"}},{"cell_type":"code","source":["#Text preprocessing layer\n","preprocessing_model = hub.KerasLayer(preprocess_url)\n","#Text Preprocessing using BERT\n","preprocessed_text = preprocessing_model(df[\"review\"])\n","preprocessed_text.keys()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iva7dZrOmTqr","executionInfo":{"status":"ok","timestamp":1664460948908,"user_tz":-330,"elapsed":5337,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"37a855c4-4cbc-4b87-aba3-380bb7354d17"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"]},{"output_type":"execute_result","data":{"text/plain":["dict_keys(['input_word_ids', 'input_mask', 'input_type_ids'])"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["So the preprocessing layer preprocessed the textual data and produced a dictionary as output containing three dictionary elements such as ​​'input_mask', 'input_word_ids', 'input_type_ids'.  Now let's understand each of the individual elements of this dictionary. "],"metadata":{"id":"Lv0RoHcEmWtk"}},{"cell_type":"code","source":["#Input_mask\n","preprocessed_text['input_mask']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DlvEYwbBmctv","executionInfo":{"status":"ok","timestamp":1664460948909,"user_tz":-330,"elapsed":16,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"df85a6b0-7924-41e2-db2c-3c166d2d6668"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(996, 128), dtype=int32, numpy=\n","array([[1, 1, 1, ..., 0, 0, 0],\n","       [1, 1, 1, ..., 0, 0, 0],\n","       [1, 1, 1, ..., 0, 0, 0],\n","       ...,\n","       [1, 1, 1, ..., 0, 0, 0],\n","       [1, 1, 1, ..., 0, 0, 0],\n","       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","source":["The first element of the dictionary is 'input_mask'. The shape of the 'input_mask' is (996,128) because there are 996 reviews and the maximum length of the sentence is 128."],"metadata":{"id":"RUcp2r_AmfKC"}},{"cell_type":"code","source":["#Input_type_ids\n","preprocessed_text[\"input_type_ids\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PzXR_gHKmhiT","executionInfo":{"status":"ok","timestamp":1664460948909,"user_tz":-330,"elapsed":14,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"46c493a0-6ab8-427c-c744-de5a8feddb71"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(996, 128), dtype=int32, numpy=\n","array([[0, 0, 0, ..., 0, 0, 0],\n","       [0, 0, 0, ..., 0, 0, 0],\n","       [0, 0, 0, ..., 0, 0, 0],\n","       ...,\n","       [0, 0, 0, ..., 0, 0, 0],\n","       [0, 0, 0, ..., 0, 0, 0],\n","       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","source":["The second element of the dictionary is \"input_type_ids\". The input type ids are really useful if they have multiple text or sentences. The shape of the 'input_mask' is (996,128) because there are 996 reviews and the maximum length of the sentence is 128.  "],"metadata":{"id":"pxqBa9VOmkHj"}},{"cell_type":"code","source":["#input_word_ids\n","preprocessed_text['input_word_ids']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TXbAHNODmjm1","executionInfo":{"status":"ok","timestamp":1664460948909,"user_tz":-330,"elapsed":8,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"49892676-5d2d-4ae3-d642-f46c0f5264d8"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(996, 128), dtype=int32, numpy=\n","array([[  101,  2326,  2003, ...,     0,     0,     0],\n","       [  101, 12476,  2326, ...,     0,     0,     0],\n","       [  101, 13877,  2001, ...,     0,     0,     0],\n","       ...,\n","       [  101,  1045,  3825, ...,     0,     0,     0],\n","       [  101,  1996,  2028, ...,     0,     0,     0],\n","       [  101,  1045,  2428, ...,     0,     0,     0]], dtype=int32)>"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","source":["The third element of the dictionary is 'input_word_ids'. The input word ids provide individual unique word ids to each of the words and these unique ids could be ids from the vocabulary. The word id for CLS is 101 and SEP is 102. "],"metadata":{"id":"xPHTiptrmoVO"}},{"cell_type":"markdown","source":["# **BERT Embedding**"],"metadata":{"id":"qEZ7T80WmtIY"}},{"cell_type":"code","source":["import warnings\n","warnings.simplefilter(\"ignore\")"],"metadata":{"id":"LzXqvDTrnLVD","executionInfo":{"status":"ok","timestamp":1664461096490,"user_tz":-330,"elapsed":1220,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["After the preprocessing stage, we will create an encoding layer that will contain an encoder URL. So this layer will act as a function pointer taking the preprocessed text and will generate the sentence or word embedding of the preprocessed text.  Since the output of this layer is also a dictionary, we can visualize the keys of the dictionary to understand the word embedding outputs of this encoding layer. "],"metadata":{"id":"qMWR2QYumv6k"}},{"cell_type":"code","source":["#Word Embedding layer\n","embedding_model = hub.KerasLayer(encoder_url)\n","embedded_output = embedding_model(preprocessed_text)\n","embedded_output.keys()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qyjYpxVAmnuE","executionInfo":{"status":"ok","timestamp":1664461595629,"user_tz":-330,"elapsed":76576,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"6b009954-3bf4-4007-a7f3-62aefa00ccef"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['pooled_output', 'sequence_output', 'encoder_outputs', 'default'])"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["So the dictionary has three key elements i.e 'encoder_outputs','pooled_output', and 'sequence_output'. Now let's try to examine the different types of keys in the resultant dictionary. "],"metadata":{"id":"u-jzvsqQm1IJ"}},{"cell_type":"code","source":["#pooled_output\n","embedded_output['pooled_output']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ur3FbAtVmy9W","executionInfo":{"status":"ok","timestamp":1664461661228,"user_tz":-330,"elapsed":665,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"133b2f01-08e7-4627-a918-8e9d8b1444df"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(996, 768), dtype=float32, numpy=\n","array([[-0.92820835, -0.53307813, -0.98807347, ..., -0.94718045,\n","        -0.7754499 ,  0.9370414 ],\n","       [-0.8528169 , -0.35731667, -0.8730308 , ..., -0.7974772 ,\n","        -0.6556542 ,  0.9271541 ],\n","       [-0.8450236 , -0.4425657 , -0.84763074, ..., -0.85680586,\n","        -0.5416942 ,  0.93708444],\n","       ...,\n","       [-0.76845974, -0.46657893, -0.8840185 , ..., -0.79948694,\n","        -0.6214458 ,  0.9042822 ],\n","       [-0.7747236 , -0.32115898, -0.8614529 , ..., -0.72820914,\n","        -0.5732518 ,  0.86374104],\n","       [-0.8754774 , -0.41060072, -0.9194106 , ..., -0.86812526,\n","        -0.6718024 ,  0.8926041 ]], dtype=float32)>"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["First, we are going to examine the pooled output. The pooled output is the embedding for the entire sentence. The shape of the pooled output is (996, 768) provided 996 is the total number of reviews and 768 is the embedding vector size. So now we can use these vectors to perform various NLP tasks like classification, NER, etc. "],"metadata":{"id":"_w1bOFw5m56a"}},{"cell_type":"code","source":["#sequence_output\n","embedded_output['sequence_output']\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mAR-FTrZm6_m","executionInfo":{"status":"ok","timestamp":1664461665433,"user_tz":-330,"elapsed":890,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"a707b8d6-da71-414b-9898-b949dcc744b0"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(996, 128, 768), dtype=float32, numpy=\n","array([[[-0.16914892,  0.10955714,  0.06207668, ..., -0.2665729 ,\n","          0.3922215 ,  0.4045772 ],\n","        [ 0.43808356, -0.40459052,  0.49144763, ..., -0.20893475,\n","          0.20450285, -0.08185712],\n","        [-0.192302  , -0.34165952,  0.07631842, ..., -0.3218694 ,\n","          0.1680937 ,  0.57546115],\n","        ...,\n","        [ 0.21660924, -0.16680014,  0.4273146 , ...,  0.2703691 ,\n","         -0.1244054 ,  0.14376658],\n","        [ 0.10489924, -0.19473974,  0.403444  , ...,  0.27094305,\n","         -0.07457556,  0.15557198],\n","        [ 0.1582209 , -0.17861842,  0.41508353, ...,  0.31933105,\n","         -0.08395606,  0.09886038]],\n","\n","       [[-0.10519296, -0.06884409, -0.29852688, ..., -0.28938407,\n","          0.16161281,  0.23380975],\n","        [ 0.23456024, -0.2770056 ,  0.15491068, ..., -0.2441566 ,\n","          0.2038424 , -0.09215128],\n","        [ 0.81120396, -0.23502271,  0.61968386, ..., -0.02933439,\n","          0.30619276, -0.29426754],\n","        ...,\n","        [ 0.01212873, -0.27478153,  0.285413  , ..., -0.03914469,\n","         -0.04086662, -0.12102579],\n","        [-0.00550147, -0.31370005,  0.25303248, ..., -0.00281613,\n","         -0.01435696, -0.19011128],\n","        [-0.00299884, -0.2767632 ,  0.35461748, ..., -0.02942899,\n","         -0.07098558, -0.18220934]],\n","\n","       [[-0.30391228, -0.22374696, -0.39360064, ..., -0.2538376 ,\n","          0.67642367,  0.29469788],\n","        [ 0.79564995, -0.5758294 , -0.15509868, ..., -0.23005357,\n","          0.5594855 ,  0.09475857],\n","        [-0.3210246 , -0.67179215, -0.31252208, ..., -0.1627745 ,\n","          0.01058873,  0.1834158 ],\n","        ...,\n","        [ 0.28168738,  0.15869519,  0.20194125, ..., -0.0079894 ,\n","          0.17118242, -0.2000033 ],\n","        [-0.01833949, -0.10330448,  0.3188044 , ...,  0.09732936,\n","          0.33666095, -0.00242479],\n","        [-0.04988751, -0.07912063,  0.26198268, ...,  0.1320144 ,\n","          0.27928746, -0.1430136 ]],\n","\n","       ...,\n","\n","       [[-0.13114202,  0.08689927,  0.05346283, ...,  0.05265052,\n","          0.30407265,  0.41421685],\n","        [ 0.42603412, -0.02943479, -0.1248292 , ..., -0.1816771 ,\n","          0.98487073,  0.25404802],\n","        [ 0.379813  , -0.4788438 ,  0.01696232, ...,  0.07292981,\n","          0.48764923, -0.0334154 ],\n","        ...,\n","        [ 0.22494832,  0.0744914 ,  0.5580882 , ...,  0.12790968,\n","         -0.01095834,  0.04036212],\n","        [ 0.28319007, -0.09635369,  0.6397926 , ...,  0.09735204,\n","          0.05577664, -0.00812563],\n","        [ 0.4397928 , -0.11857355,  0.665892  , ...,  0.3063283 ,\n","          0.12929013, -0.11683838]],\n","\n","       [[-0.09569208, -0.00305015,  0.1105364 , ..., -0.01976435,\n","          0.39701158,  0.30135107],\n","        [ 0.19348335, -0.12691188, -0.2460615 , ...,  0.34927383,\n","          0.8714751 , -0.31379795],\n","        [-0.07155117,  0.0167948 ,  0.43986148, ...,  0.6352834 ,\n","          0.57711995, -0.19989534],\n","        ...,\n","        [-0.09600344, -0.56669945,  0.4891786 , ...,  0.71667475,\n","          0.28763318, -0.5580057 ],\n","        [ 0.5983638 ,  0.139688  ,  0.79431   , ...,  0.2583792 ,\n","          0.18199448, -0.21181822],\n","        [ 0.2456419 ,  0.05117733,  0.71156925, ...,  0.29818857,\n","          0.12642747, -0.17368731]],\n","\n","       [[ 0.2474044 ,  0.01364062, -0.31913   , ...,  0.037514  ,\n","          0.31509727,  0.11522512],\n","        [ 0.3928392 , -0.37406027, -0.52827054, ..., -0.07459005,\n","          0.7267003 , -0.06640649],\n","        [ 0.49222258, -0.5189744 , -0.46276677, ..., -0.05984006,\n","         -0.05739551, -0.33885893],\n","        ...,\n","        [ 0.34598133,  0.10754021,  0.21444497, ...,  0.28780305,\n","         -0.3228979 , -0.04724743],\n","        [ 0.38096136,  0.1400633 ,  0.12281886, ...,  0.23160738,\n","         -0.29795113, -0.02509664],\n","        [ 0.33716053,  0.19962761,  0.29130602, ...,  0.29556456,\n","         -0.31744108, -0.07916653]]], dtype=float32)>"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","source":["Now the second key element is sequence output. The sequence output is the individual word embedding vectors. The shape of the sequence output is (996, 128, 768) because there are 996 input reviews, the maximum length of the sentence is 128 along with the padding and for each of the words, there are 768 size vectors. Since this is a contextual embedding the vectors for the padding have some vectors. "],"metadata":{"id":"2qc6vwgxm6Zk"}},{"cell_type":"code","source":["#Encoder output\n","len(embedded_output['encoder_outputs'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l7ae3y39m7FF","executionInfo":{"status":"ok","timestamp":1664461670781,"user_tz":-330,"elapsed":874,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"78f11f02-1e26-4b25-bcca-ca129bb3da6d"},"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["12"]},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","source":["The last key element is encoder output. The encoder output is nothing but the output of each individual encoder in the BERT base model. The length of the encoder output is 12 because there are 12 encoding layers in the BERT base model. The shape of the encoder output is also (996, 128, 768) because there are 996 input reviews, the maximum length of the sentence is 128 along with the padding and for each of the words, there are 768 size vectors."],"metadata":{"id":"jvIId6x5nCFT"}}]}