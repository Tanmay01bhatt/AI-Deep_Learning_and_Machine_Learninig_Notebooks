{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOhh8TK5aDS8tRDORmVYeJx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Skip-gram model**"],"metadata":{"id":"VkrvL3ZI6ITo"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"LivpTXE66ArP","executionInfo":{"status":"ok","timestamp":1664466099414,"user_tz":-330,"elapsed":3,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}}},"outputs":[],"source":["import re\n","import numpy as np"]},{"cell_type":"markdown","source":["# **Preparing the data and parameters**"],"metadata":{"id":"yM5v0l_S6MB1"}},{"cell_type":"markdown","source":["The very first step will be the preparation of data, here first using the function called tokenize we will tokenize our raw strings which are part of current news affairs.  "],"metadata":{"id":"RlCS3foE6WcJ"}},{"cell_type":"code","source":["# Function to tokenize the data\n","def tokenize(text):\n","    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n","    return pattern.findall(text.lower())"],"metadata":{"id":"UclUOk716O44","executionInfo":{"status":"ok","timestamp":1664466362338,"user_tz":-330,"elapsed":488,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# data and tokens\n","data = 'British Prime Minister Boris Johnson late Tuesday named his Iraqi-born education secretary, Nadhim Zahawi, as finance minister after the shock resignation of Rishi Sunak.'\n","tokens = tokenize(data)\n","print(tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6_PDmUPQ7O1t","executionInfo":{"status":"ok","timestamp":1664466364174,"user_tz":-330,"elapsed":2,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"5f142ba6-29a1-43fa-847c-f282212a45ef"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["['british', 'prime', 'minister', 'boris', 'johnson', 'late', 'tuesday', 'named', 'his', 'iraqi', 'born', 'education', 'secretary', 'nadhim', 'zahawi', 'as', 'finance', 'minister', 'after', 'the', 'shock', 'resignation', 'of', 'rishi', 'sunak']\n"]}]},{"cell_type":"markdown","source":["As we can see using the re.compile() method we have successfully tokenized the given sentence. \n","\n","Now let’s obtain the pairs of tokens and respective IDs with the help enumerate method so that by latter by using IDs we can train the model easily and lastly retrieve respective words easily.  "],"metadata":{"id":"L6k8lDcc6c49"}},{"cell_type":"code","source":["# generating tokens and Ids\n","id_to_word = {i:x for (i, x) in enumerate(tokens)}\n","word_to_id = {x:i for (i, x) in enumerate(tokens)}"],"metadata":{"id":"ZE5Obgfu6eI6","executionInfo":{"status":"ok","timestamp":1664466367123,"user_tz":-330,"elapsed":509,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["Below we can see two generated dictionaries. "],"metadata":{"id":"-ljjm8qY6f25"}},{"cell_type":"code","source":["# grab the pairs of ID's and words\n","print(word_to_id)\n","print(id_to_word)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JCNziKlR6kre","executionInfo":{"status":"ok","timestamp":1664466370963,"user_tz":-330,"elapsed":459,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"177ab3de-1a9e-4c47-9038-aef14c9a5100"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["{'british': 0, 'prime': 1, 'minister': 17, 'boris': 3, 'johnson': 4, 'late': 5, 'tuesday': 6, 'named': 7, 'his': 8, 'iraqi': 9, 'born': 10, 'education': 11, 'secretary': 12, 'nadhim': 13, 'zahawi': 14, 'as': 15, 'finance': 16, 'after': 18, 'the': 19, 'shock': 20, 'resignation': 21, 'of': 22, 'rishi': 23, 'sunak': 24}\n","{0: 'british', 1: 'prime', 2: 'minister', 3: 'boris', 4: 'johnson', 5: 'late', 6: 'tuesday', 7: 'named', 8: 'his', 9: 'iraqi', 10: 'born', 11: 'education', 12: 'secretary', 13: 'nadhim', 14: 'zahawi', 15: 'as', 16: 'finance', 17: 'minister', 18: 'after', 19: 'the', 20: 'shock', 21: 'resignation', 22: 'of', 23: 'rishi', 24: 'sunak'}\n"]}]},{"cell_type":"markdown","source":["Now we will prepare the training data, which is just a NumPy array of tokens for the whole above data where the x array represents words and the y array represent the context words for the corresponding x word lastly we are also expanding the dimension of the generated data."],"metadata":{"id":"HVuVPl_n6hKl"}},{"cell_type":"code","source":["# training data\n","def generate_training_data(tokens, word_to_id, window_size):\n","    X, Y = [], []\n","\n","    for i in range(len(tokens)):\n","        nbr_inds = list(range(max(0, i - window_size), i)) + \\\n","                   list(range(i + 1, min(len(tokens), i + window_size + 1)))\n","        for j in nbr_inds:\n","            X.append(word_to_id[tokens[i]])\n","            Y.append(word_to_id[tokens[j]])\n","            \n","    return np.array(X), np.array(Y)"],"metadata":{"id":"nuTFCGZ767GS","executionInfo":{"status":"ok","timestamp":1664466386370,"user_tz":-330,"elapsed":457,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["Now let’s generate the data. "],"metadata":{"id":"wJrd5N-J68zw"}},{"cell_type":"code","source":["# Expanding dim\n","def expand_dims(x, y):\n","    x = np.expand_dims(x, axis=0)\n","    y = np.expand_dims(y, axis=0)\n","    return x, y"],"metadata":{"id":"K-BhTtUU6-8D","executionInfo":{"status":"ok","timestamp":1664466401552,"user_tz":-330,"elapsed":465,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# generate the x and y pair\n","x, y = generate_training_data(tokens, word_to_id, 3)\n","x, y = expand_dims(x, y)"],"metadata":{"id":"LJ3IPy1t7ASP","executionInfo":{"status":"ok","timestamp":1664466415775,"user_tz":-330,"elapsed":440,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["x, y"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R8GDvrHC7d1L","executionInfo":{"status":"ok","timestamp":1664466421576,"user_tz":-330,"elapsed":496,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"6f2598ea-cb8b-447e-f65a-34a2dc8ee9db"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[ 0,  0,  0,  1,  1,  1,  1, 17, 17, 17, 17, 17,  3,  3,  3,  3,\n","          3,  3,  4,  4,  4,  4,  4,  4,  5,  5,  5,  5,  5,  5,  6,  6,\n","          6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,\n","          9,  9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11,\n","         11, 11, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 14, 14,\n","         14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16,\n","         17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19,\n","         19, 19, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 22, 22,\n","         22, 22, 22, 23, 23, 23, 23, 24, 24, 24]]),\n"," array([[ 1, 17,  3,  0, 17,  3,  4,  0,  1,  3,  4,  5,  0,  1, 17,  4,\n","          5,  6,  1, 17,  3,  5,  6,  7, 17,  3,  4,  6,  7,  8,  3,  4,\n","          5,  7,  8,  9,  4,  5,  6,  8,  9, 10,  5,  6,  7,  9, 10, 11,\n","          6,  7,  8, 10, 11, 12,  7,  8,  9, 11, 12, 13,  8,  9, 10, 12,\n","         13, 14,  9, 10, 11, 13, 14, 15, 10, 11, 12, 14, 15, 16, 11, 12,\n","         13, 15, 16, 17, 12, 13, 14, 16, 17, 18, 13, 14, 15, 17, 18, 19,\n","         14, 15, 16, 18, 19, 20, 15, 16, 17, 19, 20, 21, 16, 17, 18, 20,\n","         21, 22, 17, 18, 19, 21, 22, 23, 18, 19, 20, 22, 23, 24, 19, 20,\n","         21, 23, 24, 20, 21, 22, 24, 21, 22, 23]]))"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["Now below we write two functions, the first is to generate the training parameters for the models and the second is just the sigmoid activation function. "],"metadata":{"id":"WpLLCBRy7B5s"}},{"cell_type":"code","source":["# Parameter initialization \n","def init_parameters(vocab_size, emb_size):\n","    wrd_emb = np.random.randn(vocab_size, emb_size) * 0.01\n","    w = np.random.randn(vocab_size, emb_size) * 0.01\n","    \n","    return wrd_emb, w"],"metadata":{"id":"WK18mqD07DWV","executionInfo":{"status":"ok","timestamp":1664466441078,"user_tz":-330,"elapsed":547,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# activation function \n","def softmax(z):\n","    return np.divide(np.exp(z), np.sum(np.exp(z), axis=0, keepdims=True) + 0.001)"],"metadata":{"id":"M89_OXwr7lhQ","executionInfo":{"status":"ok","timestamp":1664466459116,"user_tz":-330,"elapsed":8069,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["# **Forward propagation**"],"metadata":{"id":"4ePr7gE47D0v"}},{"cell_type":"markdown","source":["From the second step onward we are building the model, the model building is divided into two parts forward propagation and backward propagation. \n","\n","In forward propagation, we are using the three functions first is for forwarding pass, second is for obtaining cost function and last is to calculate the difference between predictions. "],"metadata":{"id":"DsDrPuI98Hsf"}},{"cell_type":"code","source":["# Forward propogation\n","\n","def forward(inds, params):\n","    wrd_emb, w = params\n","    word_vec = wrd_emb[inds.flatten(), :].T\n","    z = np.dot(w, word_vec)\n","    out = softmax(z)\n","    \n","    cache = inds, word_vec, w, z\n","    \n","    return out, cache\n","\n","\n","def cross_entropy(y, y_hat):\n","    m = y.shape[1]\n","    cost = -(1 / m) * np.sum(np.sum(y_hat * np.log(y + 0.001), axis=0, keepdims=True), axis=1)\n","    return cost\n","\n","\n","def dsoftmax(y, out):\n","    dl_dz = out - y\n","    \n","    return dl_dz"],"metadata":{"id":"XyawOSGS7rhU","executionInfo":{"status":"ok","timestamp":1664466586126,"user_tz":-330,"elapsed":428,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["# **Backward propogation**"],"metadata":{"id":"1hF0erhB7sGk"}},{"cell_type":"markdown","source":["The backward propagation is simple propagation where we are going to propagate the error to all states of models and will update the model state.  "],"metadata":{"id":"3y1OnZEt8Jda"}},{"cell_type":"code","source":["# Backword propogation \n","def backward(y, out, cache):\n","    inds, word_vec, w, z = cache\n","    wrd_emb, w = params\n","    \n","    dl_dz = dsoftmax(y, out)\n","    # deviding by the word_vec length to find the average\n","    dl_dw = (1/word_vec.shape[1]) * np.dot(dl_dz, word_vec.T)\n","    dl_dword_vec = np.dot(w.T, dl_dz)\n","    \n","    return dl_dz, dl_dw, dl_dword_vec\n","\n","def update(params, cache, grads, lr=0.03):\n","    inds, word_vec, w, z = cache\n","    wrd_emb, w = params\n","    dl_dz, dl_dw, dl_dword_vec = grads\n","    \n","    wrd_emb[inds.flatten(), :] -= dl_dword_vec.T * lr\n","    w -= dl_dw * lr\n","    \n","    return wrd_emb, w"],"metadata":{"id":"gnINUKPl7ui9","executionInfo":{"status":"ok","timestamp":1664466600110,"user_tz":-330,"elapsed":444,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["# **Training the model**"],"metadata":{"id":"AtfdJZ2x7xKY"}},{"cell_type":"markdown","source":["So far now we have defined the model and prepared the data, and now we will start training the model.  Below we will first define some parameters vocab_size, batch_size, etc. "],"metadata":{"id":"0bcpUn2z8OuQ"}},{"cell_type":"markdown","source":["Now we will start training the model, we will train the model for 50k epochs. The procedure is can be simply understood as the first forward propagation will be carried out and later whatever the error obtained that we will transfer it to all states of the model using the backpropagation algorithms.     "],"metadata":{"id":"HpefiBUA8Syz"}},{"cell_type":"code","source":["# Training the model\n","vocab_size = len(id_to_word)\n","\n","m = y.shape[1]\n","y_one_hot = np.zeros((vocab_size, m))\n","y_one_hot[y.flatten(), np.arange(m)] = 1\n","\n","y = y_one_hot\n","\n","\n","batch_size=256\n","embed_size = 50\n","\n","params = init_parameters(vocab_size, 50)\n","\n","costs = []\n","\n","for epoch in range(50000):\n","    epoch_cost = 0\n","    \n","    batch_inds = list(range(0, x.shape[1], batch_size))\n","    np.random.shuffle(batch_inds)\n","    \n","    for i in batch_inds:\n","        x_batch = x[:, i:i+batch_size]\n","        y_batch = y[:, i:i+batch_size]\n","        \n","        pred, cache = forward(x_batch, params)\n","        grads = backward(y_batch, pred, cache)\n","        params = update(params, cache, grads, 0.03)\n","        cost = cross_entropy(pred, y_batch)\n","        \n","        epoch_cost += np.squeeze(cost)\n","        \n","    costs.append(epoch_cost)\n","    \n","    if(epoch % 250 == 0):\n","        print(\"Cost after epoch {}: {}\".format(epoch, epoch_cost))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dVwglmN973kM","executionInfo":{"status":"ok","timestamp":1664466637687,"user_tz":-330,"elapsed":22634,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"51ca6106-595e-46fd-c8cb-9a5c6b25c1e2"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Cost after epoch 0: 3.19432228549854\n","Cost after epoch 250: 3.1832204859407445\n","Cost after epoch 500: 3.086581163872388\n","Cost after epoch 750: 2.653024735089586\n","Cost after epoch 1000: 2.3337815762741427\n","Cost after epoch 1250: 2.194864087368703\n","Cost after epoch 1500: 2.1233194073146953\n","Cost after epoch 1750: 2.08954918637509\n","Cost after epoch 2000: 2.075744475214754\n","Cost after epoch 2250: 2.0724681561095806\n","Cost after epoch 2500: 2.07142851122357\n","Cost after epoch 2750: 2.070763756286456\n","Cost after epoch 3000: 2.07450135029756\n","Cost after epoch 3250: 2.0791779612052506\n","Cost after epoch 3500: 2.0811465617364187\n","Cost after epoch 3750: 2.0840210450589645\n","Cost after epoch 4000: 2.086691384309729\n","Cost after epoch 4250: 2.087092068990985\n","Cost after epoch 4500: 2.081731342508219\n","Cost after epoch 4750: 2.07329892542344\n","Cost after epoch 5000: 2.0701366687312155\n","Cost after epoch 5250: 2.072271086662586\n","Cost after epoch 5500: 2.075905639665428\n","Cost after epoch 5750: 2.0794101084474717\n","Cost after epoch 6000: 2.0828202683469734\n","Cost after epoch 6250: 2.084834918890062\n","Cost after epoch 6500: 2.082284504107491\n","Cost after epoch 6750: 2.076171319325283\n","Cost after epoch 7000: 2.0727468336611\n","Cost after epoch 7250: 2.0719132052721063\n","Cost after epoch 7500: 2.0697320821762886\n","Cost after epoch 7750: 2.070103480612788\n","Cost after epoch 8000: 2.0744152938523364\n","Cost after epoch 8250: 2.0790763049784386\n","Cost after epoch 8500: 2.080628339790379\n","Cost after epoch 8750: 2.0776587788687007\n","Cost after epoch 9000: 2.0720692833908627\n","Cost after epoch 9250: 2.0683475224067456\n","Cost after epoch 9500: 2.0700998094074157\n","Cost after epoch 9750: 2.0757915919975845\n","Cost after epoch 10000: 2.080028200087101\n","Cost after epoch 10250: 2.082950630041922\n","Cost after epoch 10500: 2.0888303922294016\n","Cost after epoch 10750: 2.0929647045555657\n","Cost after epoch 11000: 2.0881508148710477\n","Cost after epoch 11250: 2.0800671663755623\n","Cost after epoch 11500: 2.0790245479727014\n","Cost after epoch 11750: 2.080703412660932\n","Cost after epoch 12000: 2.0778341241160554\n","Cost after epoch 12250: 2.071394460208904\n","Cost after epoch 12500: 2.0631340176935806\n","Cost after epoch 12750: 2.052171225224478\n","Cost after epoch 13000: 2.0501576930028143\n","Cost after epoch 13250: 2.06006468332221\n","Cost after epoch 13500: 2.074142050174952\n","Cost after epoch 13750: 2.084649132226122\n","Cost after epoch 14000: 2.0886344151569523\n","Cost after epoch 14250: 2.088736822296491\n","Cost after epoch 14500: 2.088787418734347\n","Cost after epoch 14750: 2.091298282848437\n","Cost after epoch 15000: 2.093463101673063\n","Cost after epoch 15250: 2.0911889378379054\n","Cost after epoch 15500: 2.084738843526026\n","Cost after epoch 15750: 2.0798678939555897\n","Cost after epoch 16000: 2.08043058757761\n","Cost after epoch 16250: 2.0846622889207738\n","Cost after epoch 16500: 2.088389056633679\n","Cost after epoch 16750: 2.0887656394233405\n","Cost after epoch 17000: 2.085716941231361\n","Cost after epoch 17250: 2.080475164581527\n","Cost after epoch 17500: 2.076150836807648\n","Cost after epoch 17750: 2.0765128483460327\n","Cost after epoch 18000: 2.0800694884267767\n","Cost after epoch 18250: 2.081993368210787\n","Cost after epoch 18500: 2.0806583530272316\n","Cost after epoch 18750: 2.078020806894013\n","Cost after epoch 19000: 2.0783266388484187\n","Cost after epoch 19250: 2.0812691029513735\n","Cost after epoch 19500: 2.0830942150774727\n","Cost after epoch 19750: 2.0825831450039027\n","Cost after epoch 20000: 2.0804577199840844\n","Cost after epoch 20250: 2.0766189006897795\n","Cost after epoch 20500: 2.0731449032654172\n","Cost after epoch 20750: 2.0719133999013923\n","Cost after epoch 21000: 2.071637670182189\n","Cost after epoch 21250: 2.071678724413606\n","Cost after epoch 21500: 2.0716125086493835\n","Cost after epoch 21750: 2.071256161543689\n","Cost after epoch 22000: 2.07218086619793\n","Cost after epoch 22250: 2.0762306250813625\n","Cost after epoch 22500: 2.0828005754253835\n","Cost after epoch 22750: 2.087676821785405\n","Cost after epoch 23000: 2.0864938715254304\n","Cost after epoch 23250: 2.078396562988033\n","Cost after epoch 23500: 2.0689889107907535\n","Cost after epoch 23750: 2.0629531650152293\n","Cost after epoch 24000: 2.062461263919247\n","Cost after epoch 24250: 2.0678168541580044\n","Cost after epoch 24500: 2.074689006197368\n","Cost after epoch 24750: 2.079562598691829\n","Cost after epoch 25000: 2.0833407119761236\n","Cost after epoch 25250: 2.087358156679613\n","Cost after epoch 25500: 2.090065267229918\n","Cost after epoch 25750: 2.090224622709018\n","Cost after epoch 26000: 2.0879885069797983\n","Cost after epoch 26250: 2.085207987852435\n","Cost after epoch 26500: 2.083800096182966\n","Cost after epoch 26750: 2.0855454308204004\n","Cost after epoch 27000: 2.0920945796368\n","Cost after epoch 27250: 2.101463028103654\n","Cost after epoch 27500: 2.105926733642795\n","Cost after epoch 27750: 2.103158503995575\n","Cost after epoch 28000: 2.0981271628258513\n","Cost after epoch 28250: 2.0922415958597\n","Cost after epoch 28500: 2.086675896013916\n","Cost after epoch 28750: 2.088024012341229\n","Cost after epoch 29000: 2.0943914558726062\n","Cost after epoch 29250: 2.0972170924928717\n","Cost after epoch 29500: 2.0956698671429512\n","Cost after epoch 29750: 2.0953526582991837\n","Cost after epoch 30000: 2.0969167039516923\n","Cost after epoch 30250: 2.0942335936147467\n","Cost after epoch 30500: 2.0845787622753935\n","Cost after epoch 30750: 2.074453348367584\n","Cost after epoch 31000: 2.068757655190449\n","Cost after epoch 31250: 2.068918962348373\n","Cost after epoch 31500: 2.0728276118331976\n","Cost after epoch 31750: 2.0760073163927695\n","Cost after epoch 32000: 2.0779601122876583\n","Cost after epoch 32250: 2.0771664529612353\n","Cost after epoch 32500: 2.0751295881131315\n","Cost after epoch 32750: 2.0774056618111434\n","Cost after epoch 33000: 2.0797007419974727\n","Cost after epoch 33250: 2.0798546368097397\n","Cost after epoch 33500: 2.0755777861596187\n","Cost after epoch 33750: 2.070602555577675\n","Cost after epoch 34000: 2.068781511349062\n","Cost after epoch 34250: 2.0704362375100205\n","Cost after epoch 34500: 2.072576212443506\n","Cost after epoch 34750: 2.0744869437547946\n","Cost after epoch 35000: 2.077365842847552\n","Cost after epoch 35250: 2.081815211050209\n","Cost after epoch 35500: 2.083108894327574\n","Cost after epoch 35750: 2.0799049793735795\n","Cost after epoch 36000: 2.075737339532654\n","Cost after epoch 36250: 2.074688231003189\n","Cost after epoch 36500: 2.075327817698279\n","Cost after epoch 36750: 2.073398098462919\n","Cost after epoch 37000: 2.070113292017207\n","Cost after epoch 37250: 2.0702745561709643\n","Cost after epoch 37500: 2.0744643305180204\n","Cost after epoch 37750: 2.0838024833875832\n","Cost after epoch 38000: 2.0986882008616194\n","Cost after epoch 38250: 2.1092641410970243\n","Cost after epoch 38500: 2.1097310771536706\n","Cost after epoch 38750: 2.102212874164782\n","Cost after epoch 39000: 2.0942012933705074\n","Cost after epoch 39250: 2.0833357073112926\n","Cost after epoch 39500: 2.0716598104975468\n","Cost after epoch 39750: 2.0688682617596337\n","Cost after epoch 40000: 2.069473013601626\n","Cost after epoch 40250: 2.0745504818733735\n","Cost after epoch 40500: 2.081137324354896\n","Cost after epoch 40750: 2.0841261223506997\n","Cost after epoch 41000: 2.0851547282319394\n","Cost after epoch 41250: 2.082113169943236\n","Cost after epoch 41500: 2.078202631401138\n","Cost after epoch 41750: 2.079448097730362\n","Cost after epoch 42000: 2.084252323363306\n","Cost after epoch 42250: 2.0874272827773983\n","Cost after epoch 42500: 2.0858874213623855\n","Cost after epoch 42750: 2.0833146084434557\n","Cost after epoch 43000: 2.0839926330741556\n","Cost after epoch 43250: 2.088110578340452\n","Cost after epoch 43500: 2.092198246057006\n","Cost after epoch 43750: 2.092746680706736\n","Cost after epoch 44000: 2.088899687540122\n","Cost after epoch 44250: 2.083747632870642\n","Cost after epoch 44500: 2.078396227709678\n","Cost after epoch 44750: 2.0757373269588495\n","Cost after epoch 45000: 2.080355098117125\n","Cost after epoch 45250: 2.088645875825275\n","Cost after epoch 45500: 2.0948304172177896\n","Cost after epoch 45750: 2.0965845321479497\n","Cost after epoch 46000: 2.0956928342492347\n","Cost after epoch 46250: 2.0942224751324465\n","Cost after epoch 46500: 2.091822894167073\n","Cost after epoch 46750: 2.0869394629913955\n","Cost after epoch 47000: 2.0818092360905487\n","Cost after epoch 47250: 2.08049575620224\n","Cost after epoch 47500: 2.083245997351716\n","Cost after epoch 47750: 2.087836659521394\n","Cost after epoch 48000: 2.0912043868117993\n","Cost after epoch 48250: 2.0929150215281913\n","Cost after epoch 48500: 2.0943416890393745\n","Cost after epoch 48750: 2.095644396424258\n","Cost after epoch 49000: 2.0961731662781773\n","Cost after epoch 49250: 2.0951642085646043\n","Cost after epoch 49500: 2.0946694425238332\n","Cost after epoch 49750: 2.095683624852839\n"]}]},{"cell_type":"markdown","source":["# **Predicting the samples**"],"metadata":{"id":"3Ik0am6F756e"}},{"cell_type":"markdown","source":["Now we will obtain the prediction on samples, the test data is simply generated by arranging the token numbers that we are going to obtain the prediction for each token. \n","\n"],"metadata":{"id":"D2_Eq4rU8YcY"}},{"cell_type":"code","source":["# generating skip grams for Id's 0 to 24\n","x_test = np.arange(vocab_size)\n","x_test = np.expand_dims(x_test, axis=0)\n","softmax_test, _ = forward(x_test, params)\n","top_sorted_inds = np.argsort(softmax_test, axis=0)[-4:,:]"],"metadata":{"id":"dyDfGF5579A-","executionInfo":{"status":"ok","timestamp":1664466654724,"user_tz":-330,"elapsed":474,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# visualizing the result \n","for input_ind in range(vocab_size):\n","    input_word = id_to_word[input_ind]\n","    output_words = [id_to_word[output_ind] for output_ind in top_sorted_inds[::-1, input_ind]]\n","    print(\"{}'s skip-grams: {}\".format(input_word, output_words))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ci5eHVW-7-jy","executionInfo":{"status":"ok","timestamp":1664466663208,"user_tz":-330,"elapsed":791,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"808c61a3-fbf3-4ea6-ca11-b466bd9fbfd5"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["british's skip-grams: ['boris', 'prime', 'minister', 'education']\n","prime's skip-grams: ['johnson', 'boris', 'british', 'minister']\n","minister's skip-grams: ['nadhim', 'his', 'education', 'minister']\n","boris's skip-grams: ['tuesday', 'late', 'johnson', 'minister']\n","johnson's skip-grams: ['named', 'tuesday', 'late', 'minister']\n","late's skip-grams: ['his', 'named', 'tuesday', 'minister']\n","tuesday's skip-grams: ['iraqi', 'his', 'named', 'late']\n","named's skip-grams: ['born', 'his', 'iraqi', 'late']\n","his's skip-grams: ['education', 'iraqi', 'named', 'born']\n","iraqi's skip-grams: ['secretary', 'education', 'his', 'named']\n","born's skip-grams: ['nadhim', 'secretary', 'education', 'named']\n","education's skip-grams: ['zahawi', 'iraqi', 'his', 'born']\n","secretary's skip-grams: ['as', 'iraqi', 'education', 'zahawi']\n","nadhim's skip-grams: ['born', 'finance', 'as', 'education']\n","zahawi's skip-grams: ['minister', 'finance', 'as', 'education']\n","as's skip-grams: ['after', 'finance', 'zahawi', 'secretary']\n","finance's skip-grams: ['the', 'after', 'nadhim', 'as']\n","minister's skip-grams: ['shock', 'the', 'as', 'after']\n","after's skip-grams: ['resignation', 'the', 'finance', 'as']\n","the's skip-grams: ['finance', 'of', 'resignation', 'shock']\n","shock's skip-grams: ['resignation', 'of', 'rishi', 'the']\n","resignation's skip-grams: ['sunak', 'shock', 'rishi', 'of']\n","of's skip-grams: ['sunak', 'shock', 'rishi', 'of']\n","rishi's skip-grams: ['sunak', 'shock', 'rishi', 'of']\n","sunak's skip-grams: ['resignation', 'of', 'rishi', 'the']\n"]}]},{"cell_type":"markdown","source":["# **Continuous bag of words**"],"metadata":{"id":"Vq2dDjK18AE0"}},{"cell_type":"markdown","source":["This method helps in completing a partial incomplete sentence by predicting the words that can be fitted into the middle of the sentence based on the surrounding context of the words. The context of prediction depends on the few words before and after the predicted word.\n","\n","The CBOW model can be simply implemented using the python-based library called gensim, below we are importing all the necessary modules and packages. "],"metadata":{"id":"2QPdxeDh8lDY"}},{"cell_type":"code","source":["# imports needed and logging\n","import gzip\n","import gensim \n","import logging\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"],"metadata":{"id":"ur4sbVNC8ldU","executionInfo":{"status":"ok","timestamp":1664466866017,"user_tz":-330,"elapsed":2050,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf"],"metadata":{"id":"IL2nJyTv9NWA","executionInfo":{"status":"ok","timestamp":1664466876307,"user_tz":-330,"elapsed":2395,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["The data that we are using for the CBOW is stored in .txt file and which is the first chapter of the famous mythological book Harry Potter.  "],"metadata":{"id":"i5oDr6t68on2"}},{"cell_type":"code","source":["get_file = tf.keras.utils.get_file('horror.txt','https://analyticsindiamag.com/wp-content/uploads/2022/07/horror.txt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"74BKmUcX8qT5","executionInfo":{"status":"ok","timestamp":1664466879355,"user_tz":-330,"elapsed":795,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"43310aef-d6cb-4823-bd34-8461c0d9e693"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://analyticsindiamag.com/wp-content/uploads/2022/07/horror.txt\n","532480/531619 [==============================] - 0s 0us/step\n","540672/531619 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"markdown","source":["The below-defined function is used to preprocess the text data line by line, the function takes the .txt file and returns the tokenized version of each line. "],"metadata":{"id":"cGKUccn38u1c"}},{"cell_type":"code","source":["def read_input(input_file):\n","    \"\"\"This method reads the input file which is in gzip format\"\"\"\n","    \n","    logging.info(\"reading file {0}...this may take a while\".format(input_file))\n","    \n","    with open(input_file, 'rb') as f:\n","        for i, line in enumerate (f): \n","\n","            if (i%10000==0):\n","                logging.info (\"read {0} reviews\".format (i))\n","            # do some pre-processing and return a list of words for each review text\n","            yield gensim.utils.simple_preprocess(line)\n","\n","# read the tokenized reviews into a list\n","# each review item becomes a serries of words\n","# so this becomes a list of lists\n","documents = list(read_input(get_file))\n","logging.info(\"Done reading data file\") "],"metadata":{"id":"DdCpX2Nf8vM6","executionInfo":{"status":"ok","timestamp":1664466937463,"user_tz":-330,"elapsed":22024,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}}},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":["Now below simply we will import the word2vec model from the gensim library and will pass the data by definition some parameters like vocab_size, window, min_count, etc and we are going to train the model for 5 epochs. "],"metadata":{"id":"OEblNzK6849a"}},{"cell_type":"code","source":["# build vocabulary and train model\n","model = gensim.models.Word2Vec(documents, size=50, window=50, min_count=2, workers=10)\n","model.train(documents,total_examples=len(documents),epochs=5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Qk33BuH86cJ","executionInfo":{"status":"ok","timestamp":1664466970783,"user_tz":-330,"elapsed":2457,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"3d29686e-9e90-4852-db70-77595a685f3a"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:gensim.models.base_any2vec:consider setting layer size to a multiple of 4 for greater performance\n","WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n","WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n","WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"]},{"output_type":"execute_result","data":{"text/plain":["(292162, 394710)"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","source":["After training the model we will obtain some predictions, now let’s calculate the similar words for some test words as below.  "],"metadata":{"id":"jLQCpNLF89Yu"}},{"cell_type":"code","source":["# similar word for given querry\n","w1 = 'harry'\n","model.wv.most_similar(positive=w1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IUfEqgqI893e","executionInfo":{"status":"ok","timestamp":1664467036832,"user_tz":-330,"elapsed":963,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"d4f685f3-2bb6-45a8-a00e-955caf8f67a3"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('flaming', 0.9916119575500488),\n"," ('lit', 0.9911414980888367),\n"," ('leaned', 0.9907791018486023),\n"," ('voices', 0.9897785782814026),\n"," ('crowded', 0.9896672964096069),\n"," ('slid', 0.9892847537994385),\n"," ('welcome', 0.9892795085906982),\n"," ('coins', 0.9883919954299927),\n"," ('clambered', 0.9883610606193542),\n"," ('sliding', 0.987615168094635)]"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["w1 = 'neighbors'\n","model.wv.most_similar(positive=w1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z1pBr5Sa8_R8","executionInfo":{"status":"ok","timestamp":1664467045840,"user_tz":-330,"elapsed":3,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"7f1abaf9-2089-4bc9-f5c5-d19a10bb2bc1"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('stools', 0.9426444172859192),\n"," ('fungi', 0.9417978525161743),\n"," ('baggy', 0.9414395093917847),\n"," ('unblinkingly', 0.9409927129745483),\n"," ('knobbly', 0.9407731294631958),\n"," ('talked', 0.9407579302787781),\n"," ('halt', 0.9398818016052246),\n"," ('coats', 0.9396645426750183),\n"," ('swiftly', 0.9396216869354248),\n"," ('chair', 0.9391561150550842)]"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["w1 = 'beans'\n","model.wv.most_similar(positive=w1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ptePPVoS9AjM","executionInfo":{"status":"ok","timestamp":1664467046767,"user_tz":-330,"elapsed":2,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"ee3a8b2a-0892-433a-8f2a-ea8cbf5cd25e"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('dudleyâ', 0.9992722868919373),\n"," ('every', 0.999259352684021),\n"," ('later', 0.9990869164466858),\n"," ('racing', 0.9989426136016846),\n"," ('themselves', 0.9989418983459473),\n"," ('however', 0.998867392539978),\n"," ('wild', 0.9988646507263184),\n"," ('leaving', 0.998769998550415),\n"," ('rather', 0.9987576007843018),\n"," ('horrible', 0.998711347579956)]"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["# similarity between two unrelated words\n","model.wv.similarity(w1=\"beans\",w2=\"neighbors\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dvTlFZov9CCu","executionInfo":{"status":"ok","timestamp":1664467049746,"user_tz":-330,"elapsed":11,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"cdf1c3ea-c2e3-4eef-9547-77b14adf40eb"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8912206"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":[],"metadata":{"id":"c2RZWOA_95Py"},"execution_count":null,"outputs":[]}]}