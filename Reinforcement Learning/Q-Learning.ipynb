{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOCvAF20QI0YvUh1NqEH7UK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jWz7Y9egEzcH","executionInfo":{"status":"ok","timestamp":1662404951412,"user_tz":-330,"elapsed":6448,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"4dc62462-1e50-42b4-9fd8-29dac1c30b5c"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install pip --upgrade --user -q --no-warn-script-location\n","!pip install numpy pandas sklearn matplotlib gym==0.17.3 --user -q --no-warn-script-location\n"]},{"cell_type":"code","source":["#import the required libraries.\n","import numpy as np\n","import gym\n","import random"],"metadata":{"id":"nHXJB41fBWTA","executionInfo":{"status":"ok","timestamp":1662404951413,"user_tz":-330,"elapsed":5,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["pip install gym[toy_text]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PmEDSboaCO50","executionInfo":{"status":"ok","timestamp":1662404954285,"user_tz":-330,"elapsed":2876,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"75fb4c49-d0bd-4e4b-8ece-dd6c4f26ba9d"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gym[toy_text] in /root/.local/lib/python3.7/site-packages (0.17.3)\n","\u001b[33mWARNING: gym 0.17.3 does not provide the extra 'toy_text'\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[toy_text]) (1.7.3)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[toy_text]) (1.21.6)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[toy_text]) (1.5.0)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /root/.local/lib/python3.7/site-packages (from gym[toy_text]) (1.5.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[toy_text]) (0.16.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","source":["pip install pygame"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3s-IYBi7CvYy","executionInfo":{"status":"ok","timestamp":1662404966500,"user_tz":-330,"elapsed":5200,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"25782b94-fd0e-41ab-c369-959ded8ab01f"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pygame in /usr/local/lib/python3.7/dist-packages (2.1.2)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","source":["import os\n","os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\""],"metadata":{"id":"xtgjJ0lYC-BZ","executionInfo":{"status":"ok","timestamp":1662404975289,"user_tz":-330,"elapsed":582,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["#create the environment usign OpenAI Gym\n","env = gym.make(\"FrozenLake-v1\")"],"metadata":{"id":"kNV3xqyTBX8B","executionInfo":{"status":"ok","timestamp":1662404992446,"user_tz":-330,"elapsed":382,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"dfb58362-bc67-4367-ef02-2cf09b057e57"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n","/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"]}]},{"cell_type":"markdown","source":["#**Creating and initailizing Q-Table**"],"metadata":{"id":"Kxpo3JZrBcoM"}},{"cell_type":"code","source":["# Get the dimensions for Q-table\n","action_size = env.action_space.n\n","state_size = env.observation_space.n\n","\n","print(f\"Action Space : {action_size} | State Space: {state_size}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"baYjxEmLBejo","executionInfo":{"status":"ok","timestamp":1662405001410,"user_tz":-330,"elapsed":388,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"4ef25d79-a400-4c2a-b60a-79335d9bd2c5"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Action Space : 4 | State Space: 16\n"]}]},{"cell_type":"code","source":["# Creating a Q-table\n","qtable = np.zeros((state_size, action_size))\n","print('Shape of Q-Table',qtable.shape)\n","print(qtable)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LubA3vBfBmrI","executionInfo":{"status":"ok","timestamp":1662405003133,"user_tz":-330,"elapsed":2,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"27801b6c-3880-4304-c5fc-e124391ffc6e"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of Q-Table (16, 4)\n","[[0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]]\n"]}]},{"cell_type":"markdown","source":["#**Initializing required Hyperparameters**"],"metadata":{"id":"PHfm2tPpBs64"}},{"cell_type":"code","source":["total_episodes = 20000        # Total episodes\n","learning_rate = 0.01           # Learning rate\n","max_steps = 99                # Max steps per episode\n","gamma = 0.95                  # Discounting rate\n","\n","# Exploration parameters\n","epsilon = 1.0                 # Exploration rate\n","max_epsilon = 1.0             # Exploration probability at start\n","min_epsilon = 0.01            # Minimum exploration probability \n","decay_rate = 0.005             # Exponential decay rate for exploration prob"],"metadata":{"id":"R7keiPY8ByuS","executionInfo":{"status":"ok","timestamp":1662405005795,"user_tz":-330,"elapsed":393,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}}},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":["#**Q-Learning Algorithm**"],"metadata":{"id":"GWMV-j4fB3t-"}},{"cell_type":"code","source":["# List of rewards\n","rewards = []\n","\n","#until learning is stopped\n","for episode in range(total_episodes):\n","    # Reset the environment\n","    state = env.reset()\n","    step = 0\n","    done = False\n","    total_rewards = 0\n","    \n","    for step in range(max_steps):\n","        #Choose an action a in the current world state (s)\n","        exp_exp_tradeoff = random.uniform(0, 1)\n","        \n","        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n","        if exp_exp_tradeoff > epsilon:\n","            action = np.argmax(qtable[state,:])\n","\n","        # Else doing a random choice --> exploration\n","        else:\n","            action = env.action_space.sample()\n","\n","        # Take the action (a) and observe the outcome state(s') and reward (r)\n","        new_state, reward, done, info = env.step(action)\n","\n","        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n","        # qtable[new_state,:] : all the actions we can take from new state\n","        qtable[state, action] = qtable[state, action] + learning_rate * \\\n","                                                        (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n","        \n","        total_rewards += reward\n","        \n","        # Our new state is state\n","        state = new_state\n","        \n","        # If done (if we're dead) : finish episode\n","        if done == True: \n","            break\n","        \n","    # Reduce epsilon (because we need less and less exploration)\n","    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode) \n","\n","    rewards.append(total_rewards)\n","\n","print(\"Score over time: \" +  str(sum(rewards)/total_episodes))\n","print(qtable)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tZFH1I2HB6s2","executionInfo":{"status":"ok","timestamp":1662405022912,"user_tz":-330,"elapsed":15440,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"c6cac859-ee9c-497d-e31c-71ecff639beb"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Score over time: 0.4377\n","[[1.62723407e-01 1.39898374e-01 1.44513280e-01 1.31705323e-01]\n"," [6.97051326e-02 7.52103310e-02 6.72947656e-02 1.41445283e-01]\n"," [1.42187802e-01 6.61073862e-02 7.20446005e-02 6.09621056e-02]\n"," [3.91046877e-02 2.87880026e-09 0.00000000e+00 0.00000000e+00]\n"," [1.85458085e-01 1.34583504e-01 1.09887784e-01 9.58488088e-02]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [1.74259820e-01 7.69178211e-02 1.04642382e-01 1.98051930e-02]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [1.10631125e-01 1.64509792e-01 1.43121046e-01 2.38574985e-01]\n"," [1.19651857e-01 3.60503303e-01 1.51857986e-01 1.09207127e-01]\n"," [3.76026980e-01 2.40661290e-01 1.49642461e-01 7.71510227e-02]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [1.28787193e-01 1.55914936e-01 4.70068993e-01 1.11469350e-01]\n"," [2.31693531e-01 3.26467761e-01 6.58888882e-01 3.22632985e-01]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"]}]},{"cell_type":"markdown","source":["#**Getting the Result**"],"metadata":{"id":"CGcsCZxKCEZR"}},{"cell_type":"code","source":["for episode in range(5):\n","    state = env.reset()\n","    step = 0\n","    done = False\n","    print(\"****************************************************\")\n","    print(\"EPISODE \", episode)\n","\n","    for step in range(max_steps):\n","        \n","        # Take the action (index) that have the maximum expected future reward given that state\n","        action = np.argmax(qtable[state,:])\n","        \n","        new_state, reward, done, info = env.step(action)\n","        \n","        if done:\n","            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n","            env.render()\n","            \n","            # We print the number of step it took.\n","            print(\"Number of steps\", step)\n","            break\n","        state = new_state\n","env.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hPLmFCt4CGTt","executionInfo":{"status":"ok","timestamp":1662405031467,"user_tz":-330,"elapsed":1055,"user":{"displayName":"Tanmay Bhatt","userId":"11681573484210707767"}},"outputId":"f73c2c98-a85f-43b4-865a-bb8f1055863c"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["****************************************************\n","EPISODE  0\n","Number of steps 12\n","****************************************************\n","EPISODE  1\n","Number of steps 62\n","****************************************************\n","EPISODE  2\n","Number of steps 7\n","****************************************************\n","EPISODE  3\n","****************************************************\n","EPISODE  4\n","Number of steps 50\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"riJWmPR-CIAD"},"execution_count":null,"outputs":[]}]}